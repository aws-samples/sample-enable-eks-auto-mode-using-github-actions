name: 'auto-mode-pipeline'
permissions:
  contents: read
  id-token: write
on:
  push:
    branches:
      - main
      - dev
      - feat/*
  workflow_dispatch:
env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET || 'eks-auto-mode-backups' }}


      
jobs:
  
  check-clusters:
    runs-on: ubuntu-latest
    outputs:
      clusters-needing-auto-mode: ${{ steps.check.outputs.clusters }}
  
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Load environment variables
      run: |
        if [ -f .env ]; then
          export $(cat .env | xargs)
          echo "AWS_REGION=$AWS_REGION" >> $GITHUB_ENV
        fi
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install AWS CLI
      run: |
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip awscliv2.zip
        sudo ./aws/install --update

    - name: Check clusters for Auto Mode
      id: check
      run: |
        
        CLUSTERS=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query 'clusters[]' --output text)
        CLUSTERS_NEEDING_AUTO_MODE=""
        
        for cluster in $CLUSTERS; do
          AUTO_MODE=$(aws eks describe-cluster --name $cluster --region ${{ env.AWS_REGION }} --query 'cluster.computeConfig.enabled' --output text 2>/dev/null || echo "false")
          
          if [ "$AUTO_MODE" != "True" ]; then
            echo "Cluster $cluster needs Auto Mode enabled"
            CLUSTERS_NEEDING_AUTO_MODE="$CLUSTERS_NEEDING_AUTO_MODE $cluster"
          else
            echo "Cluster $cluster already has Auto Mode enabled"
          fi
        done
        
        echo "clusters=$CLUSTERS_NEEDING_AUTO_MODE" >> $GITHUB_OUTPUT

    - name: Update Cluster IAM Policies
      run: |
        CLUSTERS="${{ needs.check-clusters.outputs.clusters }}"
        
        for cluster in $CLUSTERS; do
          echo "Processing cluster: $cluster"
          
          # Get cluster service role name
          CLUSTER_ROLE=$(aws eks describe-cluster --name "$cluster" --query 'cluster.roleArn' --output text | cut -d'/' -f2)
          echo "Cluster role: $CLUSTER_ROLE"
          
          # Define required policies
          POLICIES=(
            "arn:aws:iam::aws:policy/AmazonEKSComputePolicy"
            "arn:aws:iam::aws:policy/AmazonEKSBlockStoragePolicy"
            "arn:aws:iam::aws:policy/AmazonEKSLoadBalancingPolicy"
            "arn:aws:iam::aws:policy/AmazonEKSNetworkingPolicy"
            "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
          )
    
          
          # Attach policies if not already attached
          for policy in "${POLICIES[@]}"; do
            if aws iam list-attached-role-policies --role-name "$CLUSTER_ROLE" --query "AttachedPolicies[?PolicyArn=='$policy']" --output text | grep -q "$policy"; then
              echo "Policy already attached: $policy"
            else
              echo "Attaching policy: $policy"
              aws iam attach-role-policy --role-name "$CLUSTER_ROLE" --policy-arn "$policy"
            fi
          done
        done



    - name: Update Subnet Tags for Auto Mode
      run: |
        CLUSTERS="${{ needs.check-clusters.outputs.clusters }}"
        
        for cluster in $CLUSTERS; do
          echo "Processing subnets for cluster: $cluster"
          
          # Get cluster subnets
          SUBNET_IDS=$(aws eks describe-cluster --name "$cluster" --query 'cluster.resourcesVpcConfig.subnetIds[]' --output text)
          echo "Subnets: $SUBNET_IDS"
          
          # Tag each subnet appropriately
          for subnet in $SUBNET_IDS; do
            # Check if subnet is public or private
            IS_PUBLIC=$(aws ec2 describe-subnets --subnet-ids "$subnet" --query 'Subnets[0].MapPublicIpOnLaunch' --output text)
            
            if [ "$IS_PUBLIC" = "True" ]; then
              echo "Tagging public subnet $subnet for external load balancers"
              aws ec2 create-tags --resources "$subnet" --tags Key=kubernetes.io/role/elb,Value=1 || echo "Tag already exists or failed"
            else
              echo "Tagging private subnet $subnet for internal load balancers"
              aws ec2 create-tags --resources "$subnet" --tags Key=kubernetes.io/role/internal-elb,Value=1 || echo "Tag already exists or failed"
            fi
          done
        done

    - name: Create EKS Auto Node Role
      run: |
        # Check if role already exists
        echo "Setting up AmazonEKSAutoNodeRole..."
    
        # Try to create the role, ignore error if it exists
        aws iam create-role --role-name AmazonEKSAutoNodeRole --assume-role-policy-document '{
          "Version": "2012-10-17",
          "Statement": [{
            "Effect": "Allow",
            "Principal": {"Service": "ec2.amazonaws.com"},
            "Action": "sts:AssumeRole"
          }]
        }' 2>/dev/null || echo "Role already exists or creation failed"
        
        # Always try to attach policies (ignore errors if already attached)
        aws iam attach-role-policy --role-name AmazonEKSAutoNodeRole --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly 2>/dev/null || true
        aws iam attach-role-policy --role-name AmazonEKSAutoNodeRole --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodeMinimalPolicy 2>/dev/null || true
        aws iam attach-role-policy --role-name AmazonEKSAutoNodeRole --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore 2>/dev/null || true
        aws iam attach-role-policy --role-name AmazonEKSAutoNodeRole --policy-arn arn:aws:iam::aws:policy/AmazonSSMPatchAssociation 2>/dev/null || true
        
        echo "Role setup completed"

     
        
  backup-and-check:
    runs-on: ubuntu-latest
    needs: check-clusters
    if: ${{ needs.check-clusters.outputs.clusters-needing-auto-mode != '' }}
    outputs:
      backup_path: ${{ steps.backup.outputs.backup_path }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Make scripts executable
        run: chmod +x scripts/*.sh
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Install tools
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
      
     
      - name: Backup cluster state
        
        id: backup
        run: |
          CLUSTERS="${{ needs.check-clusters.outputs.clusters-needing-auto-mode }}"
          ./scripts/backup-cluster-state.sh "$CLUSTERS"
          echo "backup_path=backup/$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT


           
  gradual-migration:
    needs: [check-clusters, backup-and-check]
    if: ${{ always() && needs.check-clusters.outputs.clusters != '[]'}}
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
   
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Process clusters
        run: |
          # echo "Enabling Auto Mode on ${{ matrix.cluster }} while keeping existing nodes"
          
          CLUSTERS="${{ needs.check-clusters.outputs.clusters-needing-auto-mode }}"
          NODE_ROLE_ARN="$(aws iam get-role --role-name AmazonEKSAutoNodeRole --query 'Role.Arn' --output text)"
          for cluster in $CLUSTERS; do
            aws eks update-kubeconfig --name $cluster --region ${{ env.AWS_REGION }}
            echo "ðŸ”„ Enabling Auto Mode on cluster $cluster..."
             if aws eks update-cluster-config --name "$cluster" \
              --compute-config "{\"enabled\":true, \"nodeRoleArn\": \"$NODE_ROLE_ARN\", \"nodePools\": [\"general-purpose\"]}" \
              --kubernetes-network-config '{"elasticLoadBalancing":{"enabled":true}}' \
              --storage-config '{"blockStorage":{"enabled":true}}' \
              --region ${{ env.AWS_REGION }}; then
               echo "âœ… Auto Mode enablement initiated for $cluster"
             else
               echo "âŒ Failed to enable Auto Mode for $cluster"
               exit 1
             fi
             echo "Waiting for Auto Mode to be enabled on $cluster..."
              timeout 600 bash -c '
                while true; do
                  STATUS=$(aws eks describe-cluster --name '$cluster' --region ${{ env.AWS_REGION }} --query "cluster.computeConfig.enabled" --output text)
                  if [ "$STATUS" = "True" ]; then
                    echo "Auto Mode successfully enabled on '$cluster'!"
                    break
                  fi
                  echo "Current status: $STATUS. Waiting..."
                  sleep 30
                done
              '
            echo "Waiting for Auto Mode to provision new nodes"
            sleep 300  # Give Auto Mode time to provision nodes
            
            if ! kubectl auth can-i get nodes >/dev/null 2>&1; then
                echo "âš ï¸ Skipping node operations for $ng - No kubectl permissions"

            else
              echo "Gradually draining old node groups for $cluster"
              NODEGROUPS=$(aws eks list-nodegroups --cluster-name $cluster --query 'nodegroups[]' --output text)
              for ng in $NODEGROUPS; do
                echo "Processing node group: $ng"
                # Get nodes in this node group
                NODES=$(kubectl get nodes -l eks.amazonaws.com/nodegroup=$ng -o name)
                
                for node in $NODES; do
                  echo "Cordoning $node"
                  kubectl cordon $node
                  
                  echo "Draining $node with grace period"
                  kubectl drain $node --ignore-daemonsets --delete-emptydir-data --grace-period=300 --timeout=600s || true
                  
                  # Wait between nodes to ensure workloads can reschedule
                  sleep 60
                done
                echo "Deleting node group $ng"
                aws eks delete-nodegroup --cluster-name $cluster --nodegroup-name $ng
                aws eks wait nodegroup-deleted --cluster-name $cluster --nodegroup-name $ng
                kubectl get deployments --all-namespaces -o json | \
                jq '.items[] | "\(.metadata.namespace) \(.metadata.name)"' -r | \
                while read namespace name; do
                  kubectl patch deployment $name -n $namespace -p '{"spec":{"template":{"spec":{"nodeSelector":{"eks.amazonaws.com/compute-type":"auto"}}}}}'
                done
              done
            fi 
          echo "MIGRATION_SUCCESS=true" >> $GITHUB_ENV
          echo "Completed processing cluster: $cluster"
          done
      - name: Clean Up Scaling Components
        if: env.MIGRATION_SUCCESS == 'true'
        run: |
          echo "Removing scaling components after migration"
          # Remove Cluster Autoscaler
          kubectl delete deployment cluster-autoscaler -n kube-system || echo "Cluster Autoscaler not found"

          # Remove Karpenter if it exists (since Auto Mode replaces it)
          helm uninstall karpenter -n karpenter || echo "Karpenter not found"
          kubectl delete namespace karpenter || echo "Karpenter namespace not found"
      - name: Verify Migration
        run: |
           CLUSTERS="${{ needs.check-clusters.outputs.clusters-needing-auto-mode }}"
           for cluster in $CLUSTERS; do
            echo "Verifying successful migration for $cluster"
            aws eks describe-cluster --name "$cluster" --query 'cluster.computeConfig' --output table
           done
